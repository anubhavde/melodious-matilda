{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Cornell BirdCall audio Recognition using JAX/FLAX**","metadata":{}},{"cell_type":"markdown","source":"### **Import necessary modules and setup Jupyter notebook environment**","metadata":{}},{"cell_type":"code","source":"import os, sys, time, math, gc, functools\nimport random, librosa, cv2, requests\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom typing import Any\nfrom tqdm import tqdm, tqdm_notebook\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torchvision import transforms, utils\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import Dataset, random_split, DataLoader\n\nimport jax, optax, jax.nn\nimport flax.linen as nn\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom flax.training import train_state\nfrom jax.config import config\n\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()\nwarnings.filterwarnings(\"ignore\")\n\nseed = 1234\nnp.random.seed(seed)\n\n# to suppress warnings caused by cuda version\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:36:05.572621Z","iopub.execute_input":"2022-07-21T14:36:05.573313Z","iopub.status.idle":"2022-07-21T14:36:16.535078Z","shell.execute_reply.started":"2022-07-21T14:36:05.573264Z","shell.execute_reply":"2022-07-21T14:36:16.533697Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### **TPU Detection & Configuration**","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\nexcept ValueError:\n    tpu = None\n    \nif tpu:\n    if 'TPU_DRIVER_MODE' not in globals():\n        url = 'http:' + os.environ['TPU_NAME'].split(':')[1] + ':8475/requestversion/tpu_driver_nightly'\n        resp = requests.post(url)\n        TPU_DRIVER_MODE = 1\n        \n    config.FLAGS.jax_xla_backend = \"tpu_driver\"\n    config.FLAGS.jax_backend_target = os.environ['TPU_NAME']\n    print('Registered TPU: ', config.FLAGS.jax_backend_target)\nelse:\n    print('No TPU detected!')\n    \nprint(jax.devices())","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:36:16.537385Z","iopub.execute_input":"2022-07-21T14:36:16.538059Z","iopub.status.idle":"2022-07-21T14:36:16.574240Z","shell.execute_reply.started":"2022-07-21T14:36:16.538022Z","shell.execute_reply":"2022-07-21T14:36:16.572923Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"No TPU detected!\n[CpuDevice(id=0)]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Load & Preprocess dataset**","metadata":{}},{"cell_type":"code","source":"# seeding function for reproducibility\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONAHSHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:36:16.575707Z","iopub.execute_input":"2022-07-21T14:36:16.576055Z","iopub.status.idle":"2022-07-21T14:36:16.582974Z","shell.execute_reply.started":"2022-07-21T14:36:16.576024Z","shell.execute_reply":"2022-07-21T14:36:16.581907Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"ROOT = \"../input/birdsong-recognition\"\nos.listdir(ROOT)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:36:16.585372Z","iopub.execute_input":"2022-07-21T14:36:16.586427Z","iopub.status.idle":"2022-07-21T14:36:16.601674Z","shell.execute_reply.started":"2022-07-21T14:36:16.586308Z","shell.execute_reply":"2022-07-21T14:36:16.600491Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['example_test_audio_metadata.csv',\n 'sample_submission.csv',\n 'example_test_audio',\n 'train_audio',\n 'train.csv',\n 'test.csv',\n 'example_test_audio_summary.csv']"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv(os.path.join(ROOT, 'train.csv'))[['ebird_code', 'filename', 'duration']]\ndf['path'] = ROOT + 'train_audio/' + df['ebird_code'] + \"/\" + df['filename']\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:36:16.603154Z","iopub.execute_input":"2022-07-21T14:36:16.604158Z","iopub.status.idle":"2022-07-21T14:36:17.150349Z","shell.execute_reply.started":"2022-07-21T14:36:16.604101Z","shell.execute_reply":"2022-07-21T14:36:17.149194Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"  ebird_code      filename  duration  \\\n0     aldfly  XC134874.mp3        25   \n1     aldfly  XC135454.mp3        36   \n2     aldfly  XC135455.mp3        39   \n3     aldfly  XC135456.mp3        33   \n4     aldfly  XC135457.mp3        36   \n\n                                                path  \n0  ../input/birdsong-recognitiontrain_audio/aldfl...  \n1  ../input/birdsong-recognitiontrain_audio/aldfl...  \n2  ../input/birdsong-recognitiontrain_audio/aldfl...  \n3  ../input/birdsong-recognitiontrain_audio/aldfl...  \n4  ../input/birdsong-recognitiontrain_audio/aldfl...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ebird_code</th>\n      <th>filename</th>\n      <th>duration</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aldfly</td>\n      <td>XC134874.mp3</td>\n      <td>25</td>\n      <td>../input/birdsong-recognitiontrain_audio/aldfl...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aldfly</td>\n      <td>XC135454.mp3</td>\n      <td>36</td>\n      <td>../input/birdsong-recognitiontrain_audio/aldfl...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>aldfly</td>\n      <td>XC135455.mp3</td>\n      <td>39</td>\n      <td>../input/birdsong-recognitiontrain_audio/aldfl...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>aldfly</td>\n      <td>XC135456.mp3</td>\n      <td>33</td>\n      <td>../input/birdsong-recognitiontrain_audio/aldfl...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aldfly</td>\n      <td>XC135457.mp3</td>\n      <td>36</td>\n      <td>../input/birdsong-recognitiontrain_audio/aldfl...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"SEED = 42\nFRAC = 0.2    #\nSR = 44100\nMAXLEN = 60\nN_MELS = 128\n\nseed_everything(seed)\ndevice = torch.device('cpu')\n\nclasses = set(random.sample(df['ebird_code'].unique().tolist(), 15))\nprint(classes)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:36:17.151923Z","iopub.execute_input":"2022-07-21T14:36:17.152410Z","iopub.status.idle":"2022-07-21T14:36:17.165201Z","shell.execute_reply.started":"2022-07-21T14:36:17.152376Z","shell.execute_reply":"2022-07-21T14:36:17.163794Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{'comnig', 'whtspa', 'wesblu', 'baleag', 'brthum', 'pinwar', 'herthr', 'btywar', 'pilwoo', 'banswa', 'bulori', 'stejay', 'amecro', 'canwar', 'amerob'}\n","output_type":"stream"}]},{"cell_type":"code","source":"df = df[df.ebird_code.apply(lambda x: x in classes)].reset_index(drop=True)\nkeys = set(df.ebird_code)\nvalues = np.arange(0, len(keys))\ncode_dict = dict(zip(sorted(keys), values))\ndf['label'] = df['ebird_code'].apply(lambda x: code_dict[x])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:36:17.166719Z","iopub.execute_input":"2022-07-21T14:36:17.167552Z","iopub.status.idle":"2022-07-21T14:36:17.192889Z","shell.execute_reply.started":"2022-07-21T14:36:17.167496Z","shell.execute_reply":"2022-07-21T14:36:17.191715Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"  ebird_code      filename  duration  \\\n0     amecro  XC109768.mp3        16   \n1     amecro  XC112598.mp3       126   \n2     amecro  XC112829.mp3       135   \n3     amecro  XC114550.mp3        17   \n4     amecro  XC114551.mp3        11   \n\n                                                path  label  \n0  ../input/birdsong-recognitiontrain_audio/amecr...      0  \n1  ../input/birdsong-recognitiontrain_audio/amecr...      0  \n2  ../input/birdsong-recognitiontrain_audio/amecr...      0  \n3  ../input/birdsong-recognitiontrain_audio/amecr...      0  \n4  ../input/birdsong-recognitiontrain_audio/amecr...      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ebird_code</th>\n      <th>filename</th>\n      <th>duration</th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>amecro</td>\n      <td>XC109768.mp3</td>\n      <td>16</td>\n      <td>../input/birdsong-recognitiontrain_audio/amecr...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>amecro</td>\n      <td>XC112598.mp3</td>\n      <td>126</td>\n      <td>../input/birdsong-recognitiontrain_audio/amecr...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>amecro</td>\n      <td>XC112829.mp3</td>\n      <td>135</td>\n      <td>../input/birdsong-recognitiontrain_audio/amecr...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>amecro</td>\n      <td>XC114550.mp3</td>\n      <td>17</td>\n      <td>../input/birdsong-recognitiontrain_audio/amecr...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>amecro</td>\n      <td>XC114551.mp3</td>\n      <td>11</td>\n      <td>../input/birdsong-recognitiontrain_audio/amecr...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class BirdSoundDataset(Dataset):\n    \"\"\"Bird Sound dataset.\"\"\"\n\n    def __init__(self, df, transform = None):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): must have ['path', 'label'] columns\n        \"\"\"\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    \n    def loadMP3(self, path, duration):\n        \"\"\"\n        Args:\n            path: path of the audio file \n        Returns:\n            mels: Melspectrogram of the given audio file \n        \"\"\"\n        try:\n            duration=5\n            samples = SR* duration\n            audio, _ = librosa.load(path, sr=SR)\n            \n            if 0 < len(audio):\n                audio, _ = librosa.effects.trim(audio)\n            if len(audio) > samples: # long enough\n                audio = audio[0:0+samples]\n            else: # pad blank\n                padding = samples - len(audio)\n                offset = padding // 2\n                y = np.pad(audio, (offset, samples - len(audio) - offset), 'constant')\n\n            mels = librosa.feature.melspectrogram(y=audio, sr=SR,n_mels=N_MELS, hop_length = 347,n_fft = N_MELS *20,fmin = 20, fmax = SR//2)\n            mels = librosa.power_to_db(mels).astype(np.float32)\n            mels = mels.transpose()\n            eps = 0.001\n            if np.std(mels) != 0:\n                mels = (mels - np.mean(mels)) / np.std(mels)\n            else:\n                mels = (mels - np.mean(mels)) / eps\n            return mels\n            \n        except Exception as e:\n            print(\"Error encountered while parsing file: \", path, e)\n            mels = np.zeros((N_MELS, MAXLEN*SR//347), dtype=np.float32)\n            return mels\n            \n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        path = self.df['path'].iloc[idx]\n    \n        duration=5\n        if os.path.exists(\"./\"+path.split('/')[-1]+\".npy\"):\n            mels = np.load(\"./\"+path.split('/')[-1]+\".npy\")\n        else:\n            \n            mels = self.loadMP3(path, duration)\n            np.save(\"./\"+path.split('/')[-1]+\".npy\", mels)\n        label  = self.df['label'].iloc[idx]\n        mels = np.resize(mels,(636,128,1))\n        return mels, label","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:36:17.194781Z","iopub.execute_input":"2022-07-21T14:36:17.195136Z","iopub.status.idle":"2022-07-21T14:36:17.211904Z","shell.execute_reply.started":"2022-07-21T14:36:17.195097Z","shell.execute_reply":"2022-07-21T14:36:17.210705Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Dividing the dataset into train and validation sets\ndf = df.sample(frac=1).reset_index(drop=True)\ntrain_len = int(len(df) * (1-FRAC))\ntrain_df = df.iloc[:train_len]\nvalid_df = df.iloc[train_len:]\ntrain_df.shape, valid_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:36:17.213453Z","iopub.execute_input":"2022-07-21T14:36:17.214123Z","iopub.status.idle":"2022-07-21T14:36:17.236513Z","shell.execute_reply.started":"2022-07-21T14:36:17.214076Z","shell.execute_reply":"2022-07-21T14:36:17.235147Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"((1080, 5), (271, 5))"},"metadata":{}}]},{"cell_type":"code","source":"# prepare data loaders \n#NUM_TPUS = jax.device_count()\nBATCH_SIZE = 32\n\ntrain_loader = torch.utils.data.DataLoader(BirdSoundDataset(train_df),\n                                           batch_size=BATCH_SIZE, \n                                           num_workers=0, \n                                           shuffle=True, \n                                           drop_last = True)\n\nvalid_loader = torch.utils.data.DataLoader(BirdSoundDataset(valid_df), \n                                           batch_size=BATCH_SIZE, \n                                           num_workers=0, \n                                           shuffle=True, \n                                           drop_last = True)\n\nlen(train_loader), len(valid_loader)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:36:17.239990Z","iopub.execute_input":"2022-07-21T14:36:17.241197Z","iopub.status.idle":"2022-07-21T14:36:17.252387Z","shell.execute_reply.started":"2022-07-21T14:36:17.241136Z","shell.execute_reply":"2022-07-21T14:36:17.250949Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(33, 8)"},"metadata":{}}]},{"cell_type":"code","source":"(image_batch, label_batch) = next(iter(train_loader))\nprint(image_batch.shape)\nprint(label_batch.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:36:17.254307Z","iopub.execute_input":"2022-07-21T14:36:17.255110Z","iopub.status.idle":"2022-07-21T14:36:17.945925Z","shell.execute_reply.started":"2022-07-21T14:36:17.255060Z","shell.execute_reply":"2022-07-21T14:36:17.944493Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"torch.Size([32, 636, 128, 1])\ntorch.Size([32])\n","output_type":"stream"}]},{"cell_type":"code","source":"NUM_TPUS = jax.device_count()\n\ndef copy_dataset_to_devices(dataset, devices, num_reps=1):\n    sharded_images = []\n    sharded_labels = []\n    for _ in range(num_reps):\n        for image_batch, label_batch in tqdm(dataset, ncols=100):\n            image_batch = image_batch.detach().cpu().numpy()\n            image_batches = np.split(image_batch, NUM_TPUS, axis = 0)\n            sharded_device_images = jax.device_put_sharded(image_batches, devices)\n            sharded_images.append(sharded_device_images)\n\n            label_batch = label_batch.detach().cpu().numpy()\n            label_batches = np.split(label_batch, NUM_TPUS, axis = 0)\n            sharded_device_labels = jax.device_put_sharded(label_batches, devices)\n            sharded_labels.append(sharded_device_labels)\n\n    return sharded_images, sharded_labels\n\ndevices = jax.local_devices()\nsharded_training_images, sharded_training_labels = copy_dataset_to_devices(train_loader, devices, num_reps=10)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:36:17.947821Z","iopub.execute_input":"2022-07-21T14:36:17.948153Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|                                                                        | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a81a02f79db4b22acd7d7e86a3f5f34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                        | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd9882600c4e4876868f38383c549987"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                        | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f9ef80337c74ba7979e40dd0bcecd9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                        | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f05c43eb8e804f9698777f8c03437c00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                        | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"719a0aea82604209becea1d3aa3f2a19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                        | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1e22e6d5ffd49958abbda0b59438ab3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                        | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"109b24f0b1dd4f91b72298bcc8f17aa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                        | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f58e48a834b445b2be714107f7654b66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                        | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b60d5e431f74cfc9fb6e2a837c520ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                        | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27cecb2f92bf42d4990b2cde9529f433"}},"metadata":{}}]},{"cell_type":"code","source":"NUM_CLASSES = 15 \nclass VGG19(nn.Module):\n    @nn.compact\n    def __call__(self, x, training):\n        x = self._stack(x, 64, training)\n        x = self._stack(x, 64, training)\n        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n    \n        x = self._stack(x, 128, training)\n        x = self._stack(x, 128, training)\n        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n\n        x = self._stack(x, 256, training)\n        x = self._stack(x, 256, training)\n        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))    \n\n        x = self._stack(x, 512, training)\n        x = self._stack(x, 512, training)\n        x = self._stack(x, 512, training)\n        x = self._stack(x, 512, training)\n        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))    \n    \n        x = self._stack(x, 512, training)\n        x = self._stack(x, 512, training)\n        x = self._stack(x, 512, training)\n        x = self._stack(x, 512, training)\n        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))  \n\n        x = x.reshape((x.shape[0], -1))\n\n        x = nn.Dense(features=4096)(x)\n        x = nn.BatchNorm(use_running_average=not training)(x)\n        x = nn.relu(x)\n        x = nn.Dropout(0.5, deterministic=not training)(x)\n\n        x = nn.Dense(features=4096)(x)\n        x = nn.BatchNorm(use_running_average=not training)(x)\n        x = nn.relu(x)\n        x = nn.Dropout(0.5, deterministic=not training)(x)\n    \n        x = nn.Dense(features=NUM_CLASSES)(x)\n        return x\n  \n    @staticmethod\n    def _stack(x, features, training, dropout=None):\n        x = nn.Conv(features=features, kernel_size=(3, 3), padding='SAME')(x)\n        x = nn.BatchNorm(use_running_average=not training)(x)\n        x = nn.relu(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def average_metrics(metrics):\n    '''\n    Takes the list of dictionaries of the form k: v, and returns a dictionary\n     of the form k: (average of the v).\n    '''\n    return {k: np.mean([metric[k] for metric in metrics])\n        for k in metrics[0]}\n\ndef train(initial_network_state, num_epochs):\n    '''\n    Training the model from the given state, returns the state along with the training accuracies\n    '''\n    training_accuracies = []\n    state = initial_network_state\n    for i in range(num_epochs):\n        batch_metrics = []\n        for (image_batch, label_batch) in tqdm(zip(sharded_training_images,\n                                               sharded_training_labels),\n                                           total=len(sharded_training_images),\n                                           ncols=100):\n            state, metrics = train_batch(state, image_batch, label_batch)\n            batch_metrics.append(metrics)\n        train_metrics = average_metrics(batch_metrics)\n        print(f'Epoch {i+1} done.', flush=True)\n        print(f'  Loss: {train_metrics[\"loss\"]:.4f}, '\n          + f'accuracy: {train_metrics[\"accuracy\"]:.4f}', flush=True)\n        training_accuracies.append(train_metrics[\"accuracy\"])\n    return state, training_accuracies","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VGGState(train_state.TrainState):\n    rng: Any\n    batch_stats: Any\n  \n    @classmethod\n    def create(cls, apply_fn, params, tx, rng, batch_stats):\n        opt_state = tx.init(params)\n        state = cls(0, apply_fn, params, tx, opt_state, rng, batch_stats)\n        return state\n  \n    @classmethod\n    def update_rng(cls, state, rng):\n        return VGGState.create(state.apply_fn, state.params, state.tx, rng,\n                           state.batch_stats)\n  \n    @classmethod\n    def update_batch_stats(cls, state, batch_stats):\n        return VGGState.create(state.apply_fn, state.params, state.tx,\n                           state.rng, batch_stats)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(logits, labels):\n    '''\n    Calcualtes the accuracy using the given logits and labels\n    '''\n    return jnp.mean(jnp.argmax(logits, -1) == labels)\n\ndef cross_entropy(logits, labels):\n    '''\n    Cross Entropy error between the logits and labels\n    '''\n    one_hot_labels = jax.nn.one_hot(labels, NUM_CLASSES)\n    cross_entropy = optax.softmax_cross_entropy(logits, one_hot_labels)\n    return jnp.mean(cross_entropy)\n\ndef training_loss(image_batch, label_batch, rng, batch_stats, params):\n    '''\n    Calculates the training loss \n    '''\n    logits, batch_stats = VGG19().apply({'params': params,\n                                       'batch_stats': batch_stats},\n                                      image_batch, \n                                      training=True,\n                                      rngs={'dropout': rng},\n                                      mutable=['batch_stats'])\n    loss = cross_entropy(logits, label_batch)\n    return loss, (logits, batch_stats)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@functools.partial(jax.pmap, axis_name='tpu')\ndef train_batch(state, image_batch, label_batch):\n    '''\n    Training a single batch and returns loss and the accuracy\n    '''\n    rng, subrng = jax.random.split(state.rng)\n    batch_loss_fn = functools.partial(training_loss, image_batch, label_batch,\n                                    subrng, state.batch_stats)\n    (batch_loss, (logits, batch_stats)), grads = \\\n    jax.value_and_grad(batch_loss_fn, has_aux=True)(state.params)\n  \n    gradsum = jax.lax.psum(grads, axis_name='tpu')\n\n    state = state.apply_gradients(grads=gradsum)\n    state = state.update_batch_stats(state, batch_stats['batch_stats'])\n    state = state.update_rng(state, rng)\n\n    batch_accuracy = accuracy(logits=logits, labels=label_batch)\n    batch_accuracy_sum = jax.lax.pmean(batch_accuracy, axis_name='tpu')\n    batch_loss = jax.lax.psum(batch_loss, axis_name='tpu')\n    stats = {'loss': batch_loss, 'accuracy': batch_accuracy_sum}  \n\n    return state, stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_train_state(rng, dummy_image_batch):\n    net = VGG19()\n    params = net.init({'params': rng, 'dropout': rng}, dummy_image_batch, True)\n    tx = optax.adam(learning_rate=0.01)\n    state = VGGState.create(net.apply, params['params'], tx, rng,\n                          params['batch_stats'])\n    return state","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rng = jax.random.PRNGKey(42)\nrngs = np.broadcast_to(rng, (NUM_TPUS,) + rng.shape)\nsome_dummy_image_batch = sharded_training_images[0]\nstate = jax.pmap(create_train_state, axis_name='tpu')(rngs,some_dummy_image_batch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nfinal_state, training_accuracies = train(state, num_epochs=25)\nprint(\"Total time: \", time.time() - start, \"seconds\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the Accuracy \nplt.plot(training_accuracies)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}